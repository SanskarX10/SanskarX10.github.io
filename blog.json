[
  {
    "title": "Basics of Distillation",
    "date": "2024-08-01",
    "content": "- a large trained model + a good regularizer\n- distillation - transfer its knowledge to the small model\n- large model assigns probabilities to the wrong class, less in value but still assigns but it tells us about how they generalize\n- objective_function(training) should reflect true objective , rather trained to optimize performance\n- real objective = generalize to new data\n- but this needs info about correct ways to generalize , usually unavailable\n- small model + large trained model data < same generalization as large model + small model\n- how to do it? during knowledge transfer use soft target to train the smaller model\n- large model = small model 1 + small model 2 + small model 3\n- soft target =  mean {probability_prediction (large model)} = probability_prediction (small model 1) + probability_prediction ( small_ model n)\n- $$\n H(Soft Targets)↑⟹I(Soft Targets)↑⟹σ2(Soft Targets)↓⟹Ndata↓\n$$\n* H = entropy, I = information, mu = variance = N = training data (per training case)\n* Higher entropy means the predictions are less confident (more spread out among different classes) but carry more information. For example, a soft target distribution like (0.6, 0.3, 0.1) has higher entropy than a hard target like (1, 0, 0)\n* $$\n\\ softmax(q_i) = \\frac{e^{z_i / T}}{\\sum_{j} e^{z_j / T}}\n$$\n* raising the temperature in softmax gives less confident spread out probabilities  , high temp, lower logits , smaller values , softer probability  and vice versa\n* small model train dataset = distilled model dataset\n* distilled model dataset = transfer set + soft target distribution for each case in set\n* to be continued"
  }
]
