[
  {
    "title": "Basics of Distillation",
    "date": "2024-08-01",
    "content": "# Basics of Distillation\n\n- A large trained model + a good regularizer\n- Distillation - transfer its knowledge to the small model\n- Large model assigns probabilities to the wrong class, less in value but still assigns but it tells us about how they generalize\n- Objective function (training) should reflect true objective, rather trained to optimize performance\n- Real objective = generalize to new data\n- But this needs info about correct ways to generalize, usually unavailable\n- Small model + large trained model data < same generalization as large model + small model\n- How to do it? During knowledge transfer use soft target to train the smaller model\n- Large model = small model 1 + small model 2 + small model 3\n- Soft target = mean {probability_prediction (large model)} = probability_prediction (small model 1) + probability_prediction (small_model n)\n\n$$\nH(Soft Targets)↑⟹I(Soft Targets)↑⟹σ2(Soft Targets)↓⟹N_{data}↓\n$$\n\n* H = entropy, I = information, σ2 = variance, N = training data (per training case)\n* Higher entropy means the predictions are less confident (more spread out among different classes) but carry more information. For example, a soft target distribution like (0.6, 0.3, 0.1) has higher entropy than a hard target like (1, 0, 0)\n\n$$\nsoftmax(q_i) = \\frac{e^{z_i / T}}{\\sum_{j} e^{z_j / T}}\n$$\n\n* Raising the temperature in softmax gives less confident spread out probabilities, high temp, lower logits, smaller values, softer probability and vice versa\n* Small model train dataset = distilled model dataset\n* Distilled model dataset = transfer set + soft target distribution for each case in set\n* If correct labels are known for transfer set then for significant improvement:\n  1. First objective function: cross entropy over soft target with high temperature softmax in distilled model\n  2. Second objective function: cross entropy with correct labels over same logits with normal softmax\n  3. Take weighted average of them, with lower weight on second function\n  4. It's important to multiply soft target function result with T^2 (only when using both)\n\n$$\n\\begin{equation}\n\\frac{\\partial C}{\\partial z_i} = \\frac{1}{T} (q_i - p_i) = \\frac{1}{T} \\left( \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}} - \\frac{e^{v_i/T}}{\\sum_j e^{v_j/T}} \\right)\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n\\frac{\\partial C}{\\partial z_i} \\approx \\frac{1}{T} \\left( \\frac{1 + z_i/T}{N + \\sum_j z_j/T} - \\frac{1 + v_i/T}{N + \\sum_j v_j/T} \\right)\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n\\frac{\\partial C}{\\partial z_i} \\approx \\frac{1}{NT^2} (z_i - v_i)\n\\end{equation}\n$$\n\n* Equation 1 is gradient when, Large model logits are given by vi, distilled model logits zi, soft target probabilities pi produced by vi at Temperature T\n* Equation 2 is gradient when T larger then logits in magnitude\n* Equation 3 is when logits are zero-meaned (sum of mean == 0)\n\n```python\ne = 2.718\nT = 1.5\nN = 4\nzi = [2.34, 1.64, 4.54, 3.44]\nexp_zi = [e**(i / T) for i in zi]\nqi = [exp_i / sum(exp_zi) for exp_i in exp_zi]\nvi = [0.06,0.003,1,0.6]\nexp_vi = [e**(j / T) for j in vi]\npi = [exp_v / sum(exp_vi) for exp_v in exp_vi]\ngrad_vec = [(1/T)*(i - j) for i,j in zip(qi,pi)]\n# now when T == 5, high temp\nTh = 5\nqi_ht = [(1 + (i/ Th))/(N + (sum(zi)/Th)) for i in zi]\npi_ht = [(1 + (j/ Th))/(N + (sum(vi)/Th)) for j in vi]\ngrad_vec_ht = [(1/T)*(i - j) for i,j in zip(qi_ht,pi_ht)]\n## now when logits are zero meaned\nzi_zm = [i - (sum(zi)/N) for i in zi]\nvi_zm = [j - (sum(vi)/N) for j in vi]\ngrad_vec_zm = [ (1/(N * (T**2))) * (i - j) for i,j in zip(zi, vi)]\n```\n\n* When model is too small to capture all knowledge from larger model, medium temperatures work fine, which implies that ignoring large negative logits can be helpful\n\n## Bonus\n\n### Old parallelizing method for training large ensemble models in Google\n![Old parallelizing method](/images/mermaid-flow-2.png)\n\n### New parallelizing method\n![New parallelizing method](/images/mermaid-flow-3.png)\n\n* First is splitting data second is splitting neuron for different cores"
  }
]
