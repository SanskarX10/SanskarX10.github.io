# On the Overthinking of o1-Like Models (https://arxiv.org/pdf/2412.21187)

- reasoning models have overthinking issue
- long Cot inefficieny lies in contribution to accuracy, diversity
- o1 like models produce 2-4 solution rounds for most instances, more solutions at easier level, despite token number increasing in higher level
- in more than 92% cases, first solution is correct, later solutions slightly makes an effect
- introduces - outcome efficiency metric & distinctness ratio(among Cot answers)
- outcome efficiency metric is avg sum of first correct answer tokens / total tokens multiplied by correct indicator (0 or 1)
- distinctness ratio is ratio of unique answers compared to previosly generated solution , clusters of similarity is formed by gpt4o - [1]
- distinctness ratio for first solution is 1 ( 100% distinct) upto 30% distinctness till solution 4
- process efficiency metric : avg sum of efficient token (contribution to solution diversity, independent of correctness) wrt total tokens 
- efficient token : sum of num tokens * distinct indicator (0 or 1) also used to calculate [1]
- inefficient use of generated tokens is overthinking


# (https://arxiv.org/pdf/2412.17397) 

Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning


# scaling test time compute with open models (https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)

- self refinement over subsequent iterations (need built in mechanisms for self refinement)
- search against a verifier (generate multiple, select best{verifier can be hardcoded or a reward model})
- search techniques (test-time buedget in increasing order) :
	- Best-of-N : generate multiple responses , assign score to each using reward model
	- Beam (active path) Search : used with process reward model to select top n/m steps, PRM provides sequence of scores, one for each step
	- Diverse Verifier Tree Search : split initial beams into subtree, select best per subtree

problem > N partial solutions > PRM returns probablility of each step to reach the final answer > PRM scores used by search stratergy to generate next round of steps > once search finishes > rank final solutions by prm to reach answer

Majority Voting : a simple baseline - generate N solutions pick most frequent answer

Best of N :
	- vanilla: N generation , choose max(RM reward) as answer
	- weighted: aggregate answer among identical responses , return max(total RM reward) 
		- typically ORMs should be used to reward final answer but same PRM is used here on last step
		
	
Beam Search:
	- gemerate N beams, sample N steps at temp T , score with PRM , select top N/M steps as candidate for next generation (M is beam width here) , terminate on eos or depth
	
----------------------------------------------------------------------------------------------------------------------------------------------------
	
# Training Language Models to self correct via RL (https://arxiv.org/pdf/2409.12917) 

- variants of SFT (STaR and Pair-SFT(incorrect + correct responses)) generates traces which are insufficient for instilling self correction behaviour
- model is trained to correct second attempt responses after generating an intial response
- naive RL or SFT has 
	- distribution shift: it can correct mistakes of data generated by other model but cant perform self correction
	- behavior collapse: it simply maximizes reward to get the first best answer and doesnt self correct or minimal modification in next step

- ScoRE is introduced, two stage training:
	- First-turn responses are constrained to remain close to the behavior of the base model (using KL penalty)
	- maximize reward for progress toward self correction
	- REINFORCE + KL Divergence(in stage two its for regularization) penalty for training

- Problem with standard multiturn RL
	- if we optimize second attempt's peformance, it will get good on individual attempt performance
	- but won't learn to self correct (behaviour collapse)
	- overparameterized llms will choose perfecting the first answer
	- to overcome this they needed ScoRE

- Training:
	stage I
	- keep first attempt similar to base model 
	- train model to improve on second attempt using RL
	- add penalty to not copy the first response
	stage II
	- train model to improve both first and second responses
	- add bonus reward for improvement from first response ( removing this hurts performance )
	
	
# Physics of Language Models part 2.2 for maths on GSM8K(https://arxiv.org/pdf/2408.16293)

- why models doesnt correct mistakes immedieatly during generation 
- training with mistakes can be harmful as correction and error steps are intertwined
- so training on error free data would make sense? or not ?
- full finetuning can improve a lotf ~= continued pretraining , in comparison to LoRA , error correction is different than error-free reasoning
- one mistake is computing a parameter before the computing neccesary parameters needed to compute it
- retry data for pretraining:
	- insert incorrect, uncomputable parameters in the start
	- follow each error with [BACK] token
	- control frequency of errors using retry_rate
- pretraining on retry data rarely makes errors , standard autoregressive training is fine

- Fine tuning
	- small ranks underperform on error correction
	- masking errors during fine tune can be benificial
	- LoRa is better for error detection 

- pretraining with fake mistakes
	- simple realistic fake mistakes can improve accuracy
	- fake mistakes doesnt lead to redundant solutions

------------------------------------------------------------------------------------------------------------------------------------------







